{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-cos Stable Diffusion\n",
    "\n",
    "This notebook conatins some example code to use the B-cos diffusion architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to adjust the paths. Note that the logs and checkpoints can easily require 5-10GB of space so choose the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/train.py --data_root \"/cluster/apps/vogtlab/users/nbernold/laion-ae\" --device 'cuda' --base 'configs/stable-diffusion/debugging.yaml' -t --logdir '/cluster/apps/vogtlab/users/nbernold/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/txt2img.py --config 'configs/stable-diffusion/v2-bcos-x0o-inference.yaml' --ckpt '/cluster/apps/vogtlab/users/nbernold/logs/x0o/checkpoints/last.ckpt' --device \"cuda\" --prompt 'A photo of a flamingo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructions and Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import notebook, tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "import ldm.models.diffusion.ddim \n",
    "DDIMSampler = ldm.models.diffusion.ddim.DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "from IPython.utils import io\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(\"pytorch_lightning\")\n",
    "log.propagate = False\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt=None, device=torch.device(\"cuda\"), verbose=False):\n",
    "    if ckpt == \"None\":\n",
    "        ckpt = None\n",
    "    if ckpt is not None:\n",
    "        print(f\"Loading model from {ckpt}\")\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            pl_sd = torch.load(ckpt)\n",
    "        elif device == torch.device(\"cpu\"):\n",
    "            pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "        if \"global_step\" in pl_sd:\n",
    "            print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "        sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    if ckpt is not None:\n",
    "        m, u = model.load_state_dict(sd, strict=False)\n",
    "        if len(m) > 0 and verbose:\n",
    "            print(\"missing keys:\")\n",
    "            print(m)\n",
    "        if len(u) > 0 and verbose:\n",
    "            print(\"unexpected keys:\")\n",
    "            print(u)\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        model.cuda()\n",
    "    elif device == torch.device(\"cpu\"):\n",
    "        model.cpu()\n",
    "        model.cond_stage_model.device = \"cpu\"\n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect device name. Received: {device}\")\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block some settings can be changed. Though, note that not necessarily all options are supported.\n",
    "Make sure to download the checkpoints from Huggingface and adjust the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "model_selection = \"x0\"\n",
    "\n",
    "if model_selection == \"debug\":\n",
    "    opt_config = \"configs/stable-diffusion/debugging-inference.yaml\"\n",
    "    opt_ckpt = '/cluster/apps/vogtlab/users/nbernold/logs/laion-ae2025-03-12T12-04-17_debugging/checkpoints/last.ckpt'\n",
    "\n",
    "elif model_selection == \"vanilla\": # Vanilla model used in thesis \n",
    "    opt_config = \"configs/stable-diffusion/v2-vanilla-inference.yaml\"\n",
    "    opt_ckpt = None # insert checkpoint path for vanilla.ckpt \n",
    "\n",
    "elif model_selection == \"x0\": # B-cos x0 model used in thesis \n",
    "    opt_config = \"configs/stable-diffusion/v2-bcos-x0o-inference.yaml\"\n",
    "    opt_ckpt = None # insert checkpoint path for bcos_x0.ckpt \n",
    "\n",
    "elif model_selection == \"eps\": # B-cos eps model used in thesis \n",
    "    opt_config = \"configs/stable-diffusion/v2-bcos-inference.yaml\"\n",
    "    opt_ckpt = None # insert checkpoint path for bcos_eps.ckpt \n",
    "\n",
    "opt_device = \"cuda\"\n",
    "opt_outdir = \"outputs/txt2img-samples\"\n",
    "opt_precision = \"autocast\"\n",
    "opt_bf16 = False\n",
    "opt_C, opt_H, opt_W = 6, 64, 64\n",
    "\n",
    "config = OmegaConf.load(f\"{opt_config}\")\n",
    "device = torch.device(\"cuda\") if opt_device == \"cuda\" else torch.device(\"cpu\")\n",
    "print(\"Device: \", device)\n",
    "model = load_model_from_config(config, f\"{opt_ckpt}\", device)\n",
    "\n",
    "sampler = DDIMSampler(model, device=device)\n",
    "\n",
    "os.makedirs(opt_outdir, exist_ok=True)\n",
    "outpath = opt_outdir\n",
    "\n",
    "batch_size = 1\n",
    "n_rows = 1\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    if prompt is None:\n",
    "        return\n",
    "    \n",
    "    with io.capture_output() as captured:\n",
    "        seed_everything(seed);\n",
    "\n",
    "    backup_seed = np.random.randint(0, 2147483647)\n",
    "\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "    grid_count = len(os.listdir(outpath)) - 1\n",
    "    sample_count = 0\n",
    "\n",
    "    data = [batch_size * [prompt]]\n",
    "    start_code = torch.randn([opt_n_samples, opt_C, opt_H, opt_W], device=device)\n",
    "    if model.encode_noise:\n",
    "        start_code[:,3:,:,:] = -start_code[:,:3,:,:]\n",
    "    start_code = model.mean + start_code*model.stdev\n",
    "\n",
    "    precision_scope = autocast if opt_precision==\"autocast\" or opt_bf16 else nullcontext\n",
    "    with torch.no_grad(), \\\n",
    "        precision_scope(opt_device), \\\n",
    "        model.ema_scope():\n",
    "            all_samples = list()\n",
    "            for n in trange(1, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    with io.capture_output() as captured:\n",
    "                        seed_everything(seed);\n",
    "                    if opt_scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    shape = [opt_C, opt_H, opt_W]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    if use_ddim:\n",
    "                        samples, intermediates = sampler.sample(S=opt_steps,\n",
    "                                                        conditioning=c,\n",
    "                                                        batch_size=opt_n_samples,\n",
    "                                                        shape=shape,\n",
    "                                                        verbose=False,\n",
    "                                                        unconditional_guidance_scale=opt_scale,\n",
    "                                                        unconditional_conditioning=uc,\n",
    "                                                        eta=opt_ddim_eta,\n",
    "                                                        x_T=start_code.clone(),\n",
    "                                                        log_every_t=1,\n",
    "                                                        backup_seed=(t_rem, backup_seed))\n",
    "                    else:\n",
    "                        samples, intermediates = model.sample(c, batch_size=1, return_intermediates=True, x_T=start_code,\n",
    "                        verbose=True, timesteps=None, quantize_denoised=False, mask=None, x0=None, shape=None, log_every_t=50)\n",
    "                        intermediates = {\"pred_x0\" : intermediates}\n",
    "\n",
    "                    x_samples = model.decode_first_stage(samples)\n",
    "                    sample_out = x_samples.clone()\n",
    "                    \n",
    "                    x_samples = torch.clamp(x_samples, min=0.0, max=1.0)\n",
    "\n",
    "                    for x_sample in x_samples:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        x_sample = x_sample[:,:,:3]\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                        base_count += 1\n",
    "                        sample_count += 1\n",
    "\n",
    "                    all_samples.append(x_samples)\n",
    "\n",
    "                    if False:\n",
    "                        for q in intermediates[\"pred_x0\"]: \n",
    "                            q = torch.clamp(q, min=0.0, max=1.0)\n",
    "                            all_samples.append(q)\n",
    "                    \n",
    "            if explain:\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    x_start = start_code.clone()\n",
    "                    context = model.get_learned_conditioning(prompts)\n",
    "                    \n",
    "                    with io.capture_output() as captured:\n",
    "                        seed_everything(seed);\n",
    "\n",
    "                    (_img, _cond), _fun = sampler.sample(S=opt_steps,\n",
    "                        conditioning=context,\n",
    "                        batch_size=opt_n_samples,\n",
    "                        shape=shape,\n",
    "                        verbose=False,\n",
    "                        unconditional_guidance_scale=opt_scale,\n",
    "                        unconditional_conditioning=uc,\n",
    "                        eta=opt_ddim_eta,\n",
    "                        x_T=x_start, \n",
    "                        no_grad=False,\n",
    "                        disable_tqdm=True,\n",
    "                        t_remaining=(t_rem, 0) if rest else t_rem,\n",
    "                        backup_seed=(t_rem, backup_seed),\n",
    "                        return_eps=False)\n",
    "\n",
    "                    with io.capture_output() as captured:\n",
    "                        seed_everything(seed);\n",
    "                    \n",
    "                    x_prev, pred_x0 = _fun(_img, _cond)\n",
    "                    x_samples = model.decode_first_stage(_img)\n",
    "                    y_samples = model.decode_first_stage(pred_x0)\n",
    "                    z_samples = model.decode_first_stage(x_prev)\n",
    "                    z_samples = torch.clamp(z_samples, min=0.0, max=1.0)\n",
    "                    x_samples = torch.clamp(x_samples, min=0.0, max=1.0)\n",
    "                    y_samples = torch.clamp(y_samples, min=0.0, max=1.0)\n",
    "\n",
    "                    all_samples.append(x_samples) # input\n",
    "                    all_samples.append(z_samples) # prev\n",
    "                    all_samples.append(y_samples) # pred x0\n",
    "\n",
    "                    DLW = torch.zeros((opt_C, opt_H // patchsize, opt_W // patchsize, 77, 1024), device=device)\n",
    "                    indices = [(c,i,j) for c in range(opt_C) for i in range(opt_H // patchsize) for j in range(opt_W // patchsize)]\n",
    "                    with torch.enable_grad():\n",
    "                        for c,i,j in notebook.tqdm(indices):\n",
    "                            with io.capture_output() as captured:\n",
    "                                seed_everything(seed);\n",
    "                            cond = _cond.clone().requires_grad_()\n",
    "                            with model.explanation_mode():\n",
    "                            \n",
    "                                x_prev, pred_x0 = _fun(_img, cond)\n",
    "                                ins = x_prev\n",
    "                                ins = model.decode_first_stage(ins)\n",
    "                                mid = torch.mean(ins[:,c,patchsize*i:patchsize*(i+1), patchsize*j:patchsize*(j+1)], dim=(1,2))\n",
    "                                out = mid[0]\n",
    "\n",
    "                                out.backward(inputs=[cond])\n",
    "\n",
    "                                dlw = cond.grad[0]\n",
    "                                DLW[c,i,j] = dlw\n",
    "\n",
    "                    x_samples = torch.einsum('chwij,ij->chw', DLW[:3], context[0])\n",
    "\n",
    "                    \n",
    "\n",
    "                    x_samples = x_samples.unsqueeze(0)\n",
    "\n",
    "                    x_samples -= x_samples.min()\n",
    "                    x_samples /= x_samples.abs().max()\n",
    "\n",
    "                    compx = x_samples.clone()\n",
    "\n",
    "                    x_samples = x_samples.repeat([1,2,1,1])\n",
    "\n",
    "                    x_samples = torch.nn.functional.interpolate(x_samples, scale_factor=patchsize)\n",
    "                    all_samples.append(x_samples)\n",
    "\n",
    "            # additionally, save as grid\n",
    "            grid = torch.stack(all_samples, 0)\n",
    "            grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "            grid = make_grid(grid, nrow=5)\n",
    "\n",
    "            # to image\n",
    "            grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "            grid = grid[:,:,:3]\n",
    "            grid = Image.fromarray(grid.astype(np.uint8))\n",
    "            grid.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "            grid_count += 1\n",
    "\n",
    "    if explain:\n",
    "        return DLW, context, sample_out\n",
    "    else:\n",
    "        return None, None, sample_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can adjust the settings for your generations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"a penguin\"\n",
    "\n",
    "expl_id = 2  \n",
    "use_ddim = True # strongly recommended\n",
    "opt_scale = 10.0 # unconditional guidance scale\n",
    "opt_steps = 3 # number of sampling steps\n",
    "opt_n_samples = 1 \n",
    "opt_ddim_eta = 0.0 \n",
    "explain = True # whether the model summary should be computed\n",
    "patchsize = 8 # patchsize to aggregate the output\n",
    "seed = 43 # seed\n",
    "t_rem = 2 # a number from 0 to opt_steps-1, 0 means explaining the last step, opt_steps-1 means explaining the the first step\n",
    "rest = True # if this is true, the entire denoising process after step t_rem is explained\n",
    "\n",
    "DLW, context, pred_x0 = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing of the model\n",
    "The following blocks contain various examples to create reconstructions or explanations using the model summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To identify word to token correspondences\n",
    "import open_clip\n",
    "open_clip.tokenize(prompt)\n",
    "print(list(enumerate(prompt.split(\" \"),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, name):\n",
    "        plt.imshow(img[:3].clip(0,1).permute(1,2,0).detach().cpu().numpy())\n",
    "        plt.grid(False)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/expls/'+name+'.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expl2img(expl, image):\n",
    "    print(expl.shape, image.shape)\n",
    "    dlw = expl.clone()\n",
    "    contrib = (dlw*image.squeeze()).sum(0, keepdim=True)\n",
    "    dlw = torch.nn.functional.relu(dlw)\n",
    "    dmg = dlw.clone()\n",
    "    dmg = dmg[:3]/(dmg[:3]+dmg[3:]+1e-12)\n",
    "    alpha = torch.sqrt(torch.sum(contrib**2, dim = 0)).unsqueeze(0)\n",
    "    alpha = torch.where(contrib < 0, 1e-12, alpha)\n",
    "    smooth = 0\n",
    "    if smooth:\n",
    "            alpha = torch.nn.functional.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1) // 2)\n",
    "    alpha = (alpha / torch.quantile(alpha, 0.99)).clip(0, 1)\n",
    "    #dmg = torch.cat([dmg, alpha], dim = 0).permute(1,2,0)\n",
    "    dmg = (dmg*alpha[0][None]).permute(1,2,0)\n",
    "    return dmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = model.get_learned_conditioning(prompt)[0]\n",
    "pred = pred_x0[0]\n",
    "id = expl_id\n",
    "\n",
    "name = \"expl\"\n",
    "\n",
    "DR = torch.einsum('chwji->chwj', DLW.abs()).sum(dim=(0)).reshape(64*64//patchsize//patchsize,77)\n",
    "asrt = torch.argsort(DR, dim = 1)\n",
    "asrt.shape\n",
    "DR = torch.empty(64*64//patchsize//patchsize, 77, dtype = torch.long, device=device).scatter_ (1, asrt, torch.arange (77).to(device).repeat (64*64//patchsize//patchsize, 1)).reshape(64//patchsize,64//patchsize,77)\n",
    "DRx = 1-DR[:,:,id:id+1].repeat(1,1,3)/76\n",
    "print('Strength', DRx.abs().max())\n",
    "DRx /= DRx.abs().max()\n",
    "#plt.imshow(DRx.clip(0,1).cpu().numpy())\n",
    "DRx = DRx.permute(2,0,1)\n",
    "\n",
    "# Full Reconstruction\n",
    "Rec = torch.einsum('chwji,ji->chw', DLW, context)\n",
    "save_img(Rec[:3]/Rec[:3].max(), name+\"_\"+str(seed)+\"_nrec\")\n",
    "Rec = Rec[:3]/(Rec[:3]+Rec[3:])\n",
    "save_img(Rec, name+\"_\"+str(seed)+\"_rec\")\n",
    "\n",
    "# Partial Reconstruction\n",
    "Rec = torch.einsum('chwi,i->chw', DLW[...,id,:], context[id])\n",
    "save_img(Rec[:3]/Rec[:3].max(), name+\"_\"+str(seed)+\"_partial_rec\")\n",
    "Rec = Rec[:3]/(Rec[:3]+Rec[3:])\n",
    "save_img(Rec, name+\"_\"+str(seed)+\"_partial_nrec\")\n",
    "\n",
    "# Explanation\n",
    "Rec = torch.einsum('chwi->chw', DLW[...,id,:].abs())\n",
    "Rec = Rec[:3]\n",
    "Rec /= Rec.max()\n",
    "save_img(Rec, name+\"_\"+str(seed)+\"_expl\")\n",
    "\n",
    "# Masked Explanation\n",
    "Rec *= DRx\n",
    "save_img(Rec, name+\"_\"+str(seed)+\"_masked_expl\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = model.get_learned_conditioning(prompt)[0]\n",
    "pred = pred_x0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "Rec = torch.einsum('chwji,ji->chw', DLW, context).permute(1,2,0)\n",
    "Rec = Rec[...,:3]\n",
    "Rec /= Rec.max()\n",
    "\n",
    "plt.imshow(pred[:3].permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized Reconstruction\n",
    "Rec = torch.einsum('chwji,ji->chw', DLW, context).permute(1,2,0)\n",
    "Rec = Rec[...,:3]\n",
    "Rec /= Rec.max()\n",
    "\n",
    "plt.imshow(Rec.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Reconstruction\n",
    "Rec = torch.einsum('chwji,ji->chw', DLW, context).permute(1,2,0)\n",
    "Rec = Rec[...,:3]/(Rec[...,:3]+Rec[...,3:])\n",
    "\n",
    "plt.imshow(Rec.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized Partial Reconstruction\n",
    "id = id # potentially change token\n",
    "Rec = torch.einsum('chwi,i->chw', DLW[...,id,:], context[id]).permute(1,2,0)\n",
    "Rec = Rec[...,:3]\n",
    "Rec /= Rec.max()\n",
    "\n",
    "plt.imshow(Rec.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized Partial Reconstruction\n",
    "id = id # potentially change token\n",
    "Rec = torch.einsum('chwi,i->chw', DLW[...,id,:], context[id]).permute(1,2,0)\n",
    "Rec = Rec[...,:3]/(Rec[...,:3]+Rec[...,3:])\n",
    "\n",
    "plt.imshow(Rec.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation\n",
    "id = id # potentially change token\n",
    "Expl = torch.einsum('chwi->chw', DLW[...,id,:].abs()).permute(1,2,0)\n",
    "Expl = Expl[...,:3]\n",
    "Expl /= Expl.max()\n",
    "plt.imshow(Expl.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
